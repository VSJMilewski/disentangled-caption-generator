{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Description Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook is the implementation of the baseline model for a description generation model. This model is based on an encoder-decoder model, where the encoder is a CNN and the decoder is a LSTM-RNN language model. \n",
    "\n",
    "The encoder will be pretrained on a dataset with a relative small number of labels, for the classification task. After the pretraining, both the encoder and decoder are jointly trained for the task of generating descriptions. \n",
    "\n",
    "For the current baseline a simple implementation will be used without any form of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadbars to track the run/speed\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "# numpy for arrays/matrices/mathematical stuff\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan) #will print entire matrix without dots...\n",
    "\n",
    "# nltk for tokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize   \n",
    "\n",
    "# torch for the NN stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "# torch tools for data processing\n",
    "from torch.utils.data import DataLoader\n",
    "import pycocotools #cocoAPI\n",
    "\n",
    "# torchvision for the image dataset and image processing\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "# packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# additional stuff\n",
    "import dill\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test if device has GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code many hyper parameters will be used. For instance the file locations, dimensions for the networks layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "max_epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "vocab_size = 30000\n",
    "embedding_size = 2048\n",
    "\n",
    "save_step = 100\n",
    "\n",
    "PAD = '<PAD>'\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "crop_size = 224\n",
    "transform = transforms.Compose([ \n",
    "            transforms.RandomResizedCrop(crop_size),\n",
    "            transforms.RandomHorizontalFlip(), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data from the COCO captions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.50s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.69s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "temp_data = CocoCaptions(root = '/home/victor/coco/images/train2014/',annFile = '/home/victor/coco/annotations/captions_train2014.json', transform=transforms.ToTensor())\n",
    "train_data = CocoCaptions(root = '/home/victor/coco/images/train2014/',annFile = '/home/victor/coco/annotations/captions_train2014.json', transform=transform)\n",
    "val_data = CocoCaptions(root = '/home/victor/coco/images/val2014/',annFile = '/home/victor/coco/annotations/captions_val2014.json', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary class is created to keep track of words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    def __init__(self, data, vocab_size, filename=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        if filename == None:\n",
    "            filename = 'vocab_'+str(self.vocab_size)+'.pkl'\n",
    "        self.filename = filename\n",
    "        if os.path.isfile(self.filename):\n",
    "            self.vocab, self.vocab_size, self.vocab_weight = self.load(data)\n",
    "        else: \n",
    "            self.vocab, self.vocab_size, self.vocab_weight = self.build_vocab(data)            \n",
    "        self.w2i, self.i2w = self.build_dicts()\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        \"\"\"\n",
    "        creates lookup tables to find the index given the word \n",
    "        and the otherway around \n",
    "        \"\"\"\n",
    "        w2i = defaultdict(lambda: w2i[UNK])\n",
    "        i2w = dict()\n",
    "        for i,w in enumerate(self.vocab):\n",
    "            i2w[i] = w\n",
    "            w2i[w] = i\n",
    "        return w2i, i2w\n",
    "    \n",
    "    def build_vocab(self, data): \n",
    "        \"\"\"\n",
    "        builds a vocabulary with the most occuring words, in addition to\n",
    "        the UNK token at index 1 and PAD token at index 0. \n",
    "        START and END tokens are added to the vocabulary through the\n",
    "        preprocessed sentences.\n",
    "        with vocab size none, all existing words in the data are used\n",
    "        \"\"\"\n",
    "        vocab = Counter()\n",
    "        for item in tqdm_notebook(data):\n",
    "            for sent in item[1]:\n",
    "                s = wordpunct_tokenize(sent[0].lower())\n",
    "                for w in s:\n",
    "                    vocab[w] += 1\n",
    "\n",
    "        vocab = [k for k,_ in vocab.most_common(self.vocab_size - 4)] #minus 4 because of the default tokens\n",
    "        vocab_weights = list(range(len(vocab)))\n",
    "        vocab = [PAD,UNK,START,END] + vocab # padding needs to be first, because of the math\n",
    "        vocab_weights = [0.,1.,1.,1.] + vocab_weights\n",
    "        return vocab,len(vocab), vocab_weights \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self.vocab, open(self.filename, 'wb'))\n",
    "        \n",
    "    def load(self):\n",
    "        pickle.load(vocab, open(self.filename, 'rb'))\n",
    "        vocab_size = len(vocab)\n",
    "        vocab_weights = [0.,1.,1.,1.] + list(range(len(vocab)))\n",
    "        return vocab, vocab_size, vocab_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for preparing the batch in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_batch(batch, processor):\n",
    "    \"\"\"\n",
    "    input batch: a list of tuples of sentences. \n",
    "    the lenght of the list is the number of sentences for an image. \n",
    "    the length of the tuple is the batch size.\n",
    "    \n",
    "    output batch: a tensor with for each image one of the sentences randomly chosen. \n",
    "    the first dim is the batchsize. second dim is the sentence length. \n",
    "    the sentences are padded with zeros and prefixed and post fixed with the \n",
    "    START and END token. The words are transformed to indices. \n",
    "    \"\"\"\n",
    "    chosen_sents = []\n",
    "    sent_lengths = []\n",
    "    longest = -1\n",
    "    for sample in range(len(batch[0])):\n",
    "        sentnum = np.random.choice(len(batch))\n",
    "        s = [START] + wordpunct_tokenize(batch[sentnum][sample].lower()) + [END]\n",
    "        l = len(s)\n",
    "        chosen_sents.append(s)\n",
    "        sent_lengths.append(l)\n",
    "        if longest < l:\n",
    "            longest = l\n",
    "\n",
    "    trans_batch = np.zeros((len(chosen_sents), longest))\n",
    "    for i,s in enumerate(chosen_sents):\n",
    "        trans_batch[i,:len(s)] = np.array([processor.w2i[w] for w in s])\n",
    "    batch = torch.from_numpy(trans_batch).type(torch.LongTensor).to(device)\n",
    "    sent_lengths = torch.FloatTensor(sent_lengths).to(device)\n",
    "    return batch, sent_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is a CNN which first is pretrained on the image classification task. Once pretrained, it will be used for encoding in an vector representation.\n",
    "\n",
    "This can be extended to deviding the image into a grid, where each gridcell is encoded into a vector. During decoding, an attention can then be used over the grid vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batchnorm = nn.BatchNorm1d(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # the resnet is pretrained, so turn of the gradient\n",
    "        with torch.no_grad():\n",
    "            out = self.resnet(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = self.batchnorm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is a LSTM-RNN which for each timestep generates a single word. In the first step, the hidden layer is initialized with the encoded vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "        self.target_embeddings = nn.Embedding(target_vocab_size, embedding_size)\n",
    "        self.LSTM = nn.LSTM(embedding_size, embedding_size)\n",
    "        self.logit_lin = nn.Linear(embedding_size, target_vocab_size) # out\n",
    "        \n",
    "    def forward(self, input_words, hidden_input):  \n",
    "        # find the embedding of the correct word to be predicted\n",
    "        emb = self.target_embeddings(input_words)\n",
    "        # reshape to the correct order for the LSTM\n",
    "        emb = emb.view(1,emb.size(0),self.embedding_size)\n",
    "        # Put through the next LSTM step\n",
    "        lstm_output, hidden = self.LSTM(emb, hidden_input)\n",
    "        output = self.logit_lin(lstm_output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single model is created to tie both the networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size,\n",
    "                 target_vocab_size,\n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        \n",
    "        self.encoder = EncoderCNN(embedding_size).to(device)\n",
    "        self.decoder = Decoder(target_vocab_size,embedding_size).to(device)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=0, reduce=False).to(device)\n",
    "\n",
    "    def forward(self,images, captions, caption_lengths):        \n",
    "        # Encode\n",
    "        h0 = self.encoder(images)\n",
    "        \n",
    "        #prepare decoder initial hidden state\n",
    "        h0 = h0.unsqueeze(0)\n",
    "        c0 = torch.zeros(h0.shape)\n",
    "        hidden_state = (h0,c0)\n",
    "        \n",
    "        # Decode\n",
    "        batch_size, max_sent_len = captions.shape\n",
    "        out = torch.zeros((batch_size))  \n",
    "        for w_idx in range(max_sent_len-1):\n",
    "            prediction, hidden_state = self.decoder(captions[:,w_idx].view(-1,1), hidden_state)\n",
    "            out += self.loss(prediction.squeeze(0), captions[:,w_idx+1])\n",
    "        \n",
    "        #normalize loss\n",
    "        out = torch.mean(torch.div(out,caption_lengths))  # the loss is the average of losses, so divide over number of words in each sentence\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model is initialised and the optimizer for the model is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = CaptionModel(embedding_size, vocab_size, device)\n",
    "caption_model.train(True) #probably not needed. better to be safe\n",
    "opt = SGD(caption_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dataprocessor is created. If a pickle with the given vocabsize already exists, it is loaded, otherwise a new one is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c8cb19f43a48f9bd3ea20b34f70650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=82783), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup dataloaders with train and val data\n",
    "temploader = DataLoader(dataset=temp_data, batch_size=1, shuffle=False, drop_last=False, num_workers=1)\n",
    "processor = DataProcessor(data=temploader, vocab_size=vocab_size)\n",
    "processor.save()\n",
    "del(temploader)\n",
    "del(temp_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader for processing the data for both the training and validation data are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_data, batch_size=2, shuffle=True, drop_last=True, num_workers=4)\n",
    "valloader = DataLoader(dataset=val_data, batch_size=1, shuffle=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4db1c7c6974e8883ff0411f3c3d0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c525683ad134bd2a49bb4ef95a3aee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "opt.zero_grad()\n",
    "\n",
    "#loop over number of epochs\n",
    "for it in tnrange(1):\n",
    "    batch_losses = []\n",
    "    #loop over all the training batches\n",
    "    for i_batch, (image, caption) in tqdm_notebook(enumerate(trainloader), total=len(trainloader),leave=False):\n",
    "        image = image.to(device)\n",
    "        caption, caption_lengths = transform_batch(caption, processor)\n",
    "        loss = caption_model(image, caption, caption_lengths)\n",
    "        loss.backward()\n",
    "        batch_losses.append(float(loss))\n",
    "        opt.step()\n",
    "        if i_batch == 100:\n",
    "            break\n",
    "    losses += batch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.677347183227539,\n",
       " 6.263830184936523,\n",
       " 6.010639667510986,\n",
       " 6.627231597900391,\n",
       " 5.240501403808594,\n",
       " 6.097297668457031,\n",
       " 6.538796424865723,\n",
       " 5.61591911315918,\n",
       " 5.088732719421387,\n",
       " 6.336756706237793,\n",
       " 5.212302207946777,\n",
       " 5.183477878570557,\n",
       " 6.108344078063965,\n",
       " 5.698338985443115,\n",
       " 6.346169471740723,\n",
       " 5.851977348327637,\n",
       " 5.411420822143555,\n",
       " 5.807036399841309,\n",
       " 5.518843650817871,\n",
       " 4.911383628845215,\n",
       " 5.431375503540039,\n",
       " 6.984222412109375,\n",
       " 6.465095520019531,\n",
       " 7.481247425079346,\n",
       " 6.067309379577637,\n",
       " 6.476211071014404,\n",
       " 6.563053607940674,\n",
       " 6.030815124511719,\n",
       " 6.7606964111328125,\n",
       " 6.8211259841918945,\n",
       " 5.965514183044434,\n",
       " 6.815525054931641,\n",
       " 5.64166259765625,\n",
       " 6.392036437988281,\n",
       " 6.32445764541626,\n",
       " 8.40359115600586,\n",
       " 7.272234916687012,\n",
       " 7.894472122192383,\n",
       " 7.5314788818359375,\n",
       " 5.755794525146484,\n",
       " 7.293753623962402,\n",
       " 6.357259750366211,\n",
       " 5.608423233032227,\n",
       " 8.047916412353516,\n",
       " 6.749805450439453,\n",
       " 6.04095458984375,\n",
       " 7.458600044250488,\n",
       " 7.317066669464111,\n",
       " 7.891091346740723,\n",
       " 7.4335126876831055,\n",
       " 7.325252532958984,\n",
       " 6.691862106323242,\n",
       " 7.218371868133545,\n",
       " 5.577770233154297,\n",
       " 6.2541303634643555,\n",
       " 8.445154190063477,\n",
       " 5.772397994995117,\n",
       " 7.062015056610107,\n",
       " 8.265691757202148,\n",
       " 8.535228729248047,\n",
       " 6.151697635650635,\n",
       " 6.748847961425781,\n",
       " 9.327831268310547,\n",
       " 10.164112091064453,\n",
       " 5.779773712158203,\n",
       " 5.934869289398193,\n",
       " 8.399154663085938,\n",
       " 5.048726558685303,\n",
       " 7.58206844329834,\n",
       " 7.794114112854004,\n",
       " 8.805413246154785,\n",
       " 11.260211944580078,\n",
       " 10.732141494750977,\n",
       " 8.106745719909668,\n",
       " 8.220873832702637,\n",
       " 9.437921524047852,\n",
       " 10.47317886352539,\n",
       " 6.081221580505371,\n",
       " 9.003753662109375,\n",
       " 5.918601036071777,\n",
       " 8.432456970214844,\n",
       " 8.866318702697754,\n",
       " 8.406412124633789,\n",
       " 8.579214096069336,\n",
       " 8.2822904586792,\n",
       " 8.161727905273438,\n",
       " 9.685169219970703,\n",
       " 9.965941429138184,\n",
       " 9.34447193145752,\n",
       " 9.997817993164062,\n",
       " 8.927433013916016,\n",
       " 10.200799942016602,\n",
       " 10.041826248168945,\n",
       " 12.701119422912598,\n",
       " 9.098381042480469,\n",
       " 6.78648567199707,\n",
       " 10.324538230895996,\n",
       " 6.400638580322266,\n",
       " 8.451335906982422,\n",
       " 8.800562858581543,\n",
       " 6.8847856521606445]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dump trained models\n",
    "# timestamp = datetime.now()\n",
    "# last_model_file_name = 'encmean_model_last-it_{}_t_{:%m_%d_%H_%M}.torchsave'.format(it, timestamp)\n",
    "# torch.save(encdec.state_dict(), last_model_file_name)\n",
    "\n",
    "# print('Model saved in file: {}'.format(last_model_file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
