{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Description Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook is the implementation of the baseline model for a description generation model. This model is based on an encoder-decoder model, where the encoder is a CNN and the decoder is a LSTM-RNN language model. \n",
    "\n",
    "The encoder will be pretrained on a dataset with a relative small number of labels, for the classification task. After the pretraining, both the encoder and decoder are jointly trained for the task of generating descriptions. \n",
    "\n",
    "For the current baseline a simple implementation will be used without any form of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('tokenization...')? (eval.py, line 30)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2963\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-fca4c63b0924>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pycocoevalcap.eval import COCOEvalCap\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/data/Documents/publication/multimodal-descriptions/pycocoevalcap/eval.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    print 'tokenization...'\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('tokenization...')?\n"
     ]
    }
   ],
   "source": [
    "# loadbars to track the run/speed\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "# numpy for arrays/matrices/mathematical stuff\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan) #will print entire matrix without dots...\n",
    "\n",
    "# nltk for tokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize   \n",
    "\n",
    "# torch for the NN stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "# torch tools for data processing\n",
    "from torch.utils.data import DataLoader\n",
    "import pycocotools #cocoAPI\n",
    "\n",
    "# torchvision for the image dataset and image processing\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "#coco captions evaluation\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import seaborn\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# additional stuff\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test if device has GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up file names and pathes\n",
    "annFile='~/annotations/captions_val2014.json'\n",
    "subtypes=['results', 'evalImgs', 'eval']\n",
    "# download Stanford models\n",
    "!./get_stanford_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code many hyper parameters will be used. For instance the file locations, dimensions for the networks layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "max_epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "vocab_size = 30000\n",
    "embedding_size = 2048\n",
    "\n",
    "save_step = 100\n",
    "\n",
    "PAD = '<PAD>'\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "crop_size = 224\n",
    "transform = transforms.Compose([ \n",
    "            transforms.RandomResizedCrop(crop_size),\n",
    "            transforms.RandomHorizontalFlip(), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data from the COCO captions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.70s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.75s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "temp_data = CocoCaptions(root = '/home/victor/coco/images/train2014/',annFile = '/home/victor/coco/annotations/captions_train2014.json', transform=transforms.ToTensor())\n",
    "train_data = CocoCaptions(root = '/home/victor/coco/images/train2014/',annFile = '/home/victor/coco/annotations/captions_train2014.json', transform=transform)\n",
    "val_data = CocoCaptions(root = '/home/victor/coco/images/val2014/',annFile = '/home/victor/coco/annotations/captions_val2014.json', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary class is created to keep track of words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    def __init__(self, data, vocab_size, filename=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        if filename == None:\n",
    "            filename = 'vocab_'+str(self.vocab_size)+'.pkl'\n",
    "        self.filename = filename\n",
    "        if os.path.isfile(self.filename):\n",
    "            self.vocab, self.vocab_size, self.vocab_weight = self.load()\n",
    "        else: \n",
    "            self.vocab, self.vocab_size, self.vocab_weight = self.build_vocab(data)            \n",
    "        self.w2i, self.i2w = self.build_dicts()\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        \"\"\"\n",
    "        creates lookup tables to find the index given the word \n",
    "        and the otherway around \n",
    "        \"\"\"\n",
    "        w2i = defaultdict(lambda: w2i[UNK])\n",
    "        i2w = dict()\n",
    "        for i,w in enumerate(self.vocab):\n",
    "            i2w[i] = w\n",
    "            w2i[w] = i\n",
    "        return w2i, i2w\n",
    "    \n",
    "    def build_vocab(self, data): \n",
    "        \"\"\"\n",
    "        builds a vocabulary with the most occuring words, in addition to\n",
    "        the UNK token at index 1 and PAD token at index 0. \n",
    "        START and END tokens are added to the vocabulary through the\n",
    "        preprocessed sentences.\n",
    "        with vocab size none, all existing words in the data are used\n",
    "        \"\"\"\n",
    "        vocab = Counter()\n",
    "        for item in tqdm_notebook(data):\n",
    "            for sent in item[1]:\n",
    "                s = wordpunct_tokenize(sent[0].lower())\n",
    "                for w in s:\n",
    "                    vocab[w] += 1\n",
    "\n",
    "        vocab = [k for k,_ in vocab.most_common(self.vocab_size - 4)] #minus 4 because of the default tokens\n",
    "        vocab_weights = list(range(len(vocab)))\n",
    "        vocab = [PAD,UNK,START,END] + vocab # padding needs to be first, because of the math\n",
    "        vocab_weights = [0.,1.,1.,1.] + vocab_weights\n",
    "        return vocab,len(vocab), vocab_weights \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self.vocab, open(self.filename, 'wb'))\n",
    "        \n",
    "    def load(self):\n",
    "        vocab = pickle.load(open(self.filename, 'rb'))\n",
    "        vocab_size = len(vocab)\n",
    "        vocab_weights = [0.,1.,1.,1.] + list(range(len(vocab)))\n",
    "        return vocab, vocab_size, vocab_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for preparing the batch in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_batch(batch, processor):\n",
    "    \"\"\"\n",
    "    input batch: tuple with the images and a list of tuples of sentences. \n",
    "    the lenght of the list is the number of sentences for an image. \n",
    "    the length of the tuple is the batch size.\n",
    "    \n",
    "    output batch: a tensor with for each image one of the sentences randomly chosen. \n",
    "    the first dim is the batchsize. second dim is the sentence length. \n",
    "    the sentences are padded with zeros and prefixed and post fixed with the \n",
    "    START and END token. The words are transformed to indices. \n",
    "    \"\"\"\n",
    "    sent_lengths = []\n",
    "    longest = -1\n",
    "    images,captions = batch\n",
    "    trans_images = None\n",
    "    trans_captions = []\n",
    "    repeat_size = images[0].size()\n",
    "#     repeat_size[0] *= 5\n",
    "    print(repeat_size)\n",
    "    print(images.size())\n",
    "    for sample_num in range(len(captions[0])):\n",
    "        number_of_captions = len(captions)\n",
    "        if trans_images is None:\n",
    "            print(images[sample_num].size())\n",
    "            trans_images = images[sample_num].repeat([5,3,224,224])\n",
    "            print(trans_images.size())\n",
    "        else:\n",
    "            result = torch.cat([trans_images, images[sample_num].repeat(repeat_size)],0)\n",
    "        for sentnum in range(number_of_captions):\n",
    "            s = [START] + wordpunct_tokenize(captions[sentnum][sample_num].lower()) + [END]\n",
    "            l = len(s)\n",
    "            trans_captions.append(s)\n",
    "            sent_lengths.append(l)\n",
    "            if longest < l:\n",
    "                longest = l\n",
    "\n",
    "    final_images = np.array(trans_images)\n",
    "    final_images = torch.from_numpy(final_images).type(torch.LongTensor).to(device)\n",
    "    final_captions = np.zeros((len(trans_captions), longest))\n",
    "    for i,s in enumerate(trans_captions):\n",
    "        final_captions[i,:len(s)] = np.array([processor.w2i[w] for w in s])\n",
    "    batch = torch.from_numpy(trans_batch).type(torch.LongTensor).to(device)\n",
    "    sent_lengths = torch.FloatTensor(sent_lengths).to(device)\n",
    "    return batch, sent_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is a CNN which first is pretrained on the image classification task. Once pretrained, it will be used for encoding in an vector representation.\n",
    "\n",
    "This can be extended to deviding the image into a grid, where each gridcell is encoded into a vector. During decoding, an attention can then be used over the grid vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batchnorm = nn.BatchNorm1d(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # the resnet is pretrained, so turn of the gradient\n",
    "        with torch.no_grad():\n",
    "            out = self.resnet(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = self.batchnorm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is a LSTM-RNN which for each timestep generates a single word. In the first step, the hidden layer is initialized with the encoded vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "        self.target_embeddings = nn.Embedding(target_vocab_size, embedding_size)\n",
    "        self.LSTM = nn.LSTM(embedding_size, embedding_size)\n",
    "        self.logit_lin = nn.Linear(embedding_size, target_vocab_size) # out\n",
    "        \n",
    "    def forward(self, input_words, hidden_input):  \n",
    "        # find the embedding of the correct word to be predicted\n",
    "        emb = self.target_embeddings(input_words)\n",
    "        # reshape to the correct order for the LSTM\n",
    "        emb = emb.view(1,emb.size(0),self.embedding_size)\n",
    "        # Put through the next LSTM step\n",
    "        lstm_output, hidden = self.LSTM(emb, hidden_input)\n",
    "        output = self.logit_lin(lstm_output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single model is created to tie both the networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size,\n",
    "                 target_vocab_size,\n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        \n",
    "        self.encoder = EncoderCNN(embedding_size).to(device)\n",
    "        self.decoder = Decoder(target_vocab_size,embedding_size).to(device)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=0, reduce=False).to(device)\n",
    "\n",
    "    def forward(self,images, captions, caption_lengths):        \n",
    "        # Encode\n",
    "        h0 = self.encoder(images)\n",
    "        \n",
    "        #prepare decoder initial hidden state\n",
    "        h0 = h0.unsqueeze(0)\n",
    "        c0 = torch.zeros(h0.shape)\n",
    "        hidden_state = (h0,c0)\n",
    "        \n",
    "        # Decode\n",
    "        batch_size, max_sent_len = captions.shape\n",
    "        out = torch.zeros((batch_size))  \n",
    "        for w_idx in range(max_sent_len-1):\n",
    "            prediction, hidden_state = self.decoder(captions[:,w_idx].view(-1,1), hidden_state)\n",
    "            out += self.loss(prediction.squeeze(0), captions[:,w_idx+1])\n",
    "        \n",
    "        #normalize loss\n",
    "        out = torch.mean(torch.div(out,caption_lengths))  # the loss is the average of losses, so divide over number of words in each sentence\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model is initialised and the optimizer for the model is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = CaptionModel(embedding_size, vocab_size, device)\n",
    "caption_model.train(True) #probably not needed. better to be safe\n",
    "opt = SGD(caption_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dataprocessor is created. If a pickle with the given vocabsize already exists, it is loaded, otherwise a new one is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataloaders with train and val data\n",
    "temploader = DataLoader(dataset=temp_data, batch_size=1, shuffle=False, drop_last=False, num_workers=1)\n",
    "processor = DataProcessor(data=temploader, vocab_size=vocab_size)\n",
    "processor.save()\n",
    "del(temploader)\n",
    "del(temp_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader for processing the data for both the training and validation data are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_data, batch_size=13, shuffle=True, drop_last=True, num_workers=4)\n",
    "valloader = DataLoader(dataset=val_data, batch_size=1, shuffle=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([13, 3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 422GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.c:218",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a75305219aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransform_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-fab1cc25466a>\u001b[0m in \u001b[0;36mtransform_batch\u001b[0;34m(batch, processor)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrans_images\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrans_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 422GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.c:218"
     ]
    }
   ],
   "source": [
    "for b in trainloader:\n",
    "    break\n",
    "transform_batch(b,processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation for after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_predictions(model, prediction_file_name):\n",
    "    # Use submodules for prediction\n",
    "    encoder = model.encoder\n",
    "    decoder = model.decoder\n",
    "\n",
    "    predicted_sentences = []\n",
    "\n",
    "    for s,ts in zip(source_processor_val.sentences,target_processor_val.sentences):\n",
    "        mask = torch.from_numpy(np.ones([1,len(s)])).type(torch.FloatTensor)\n",
    "        words_tokens = torch.LongTensor([source_processor.w2i[w] for w in s])\n",
    "        words_tokens_tar = torch.LongTensor([target_processor.w2i[w] for w in ts])\n",
    "        pos_tokens = torch.LongTensor([i for i in range(len(s))])\n",
    "        len_s = torch.FloatTensor([len(s)])\n",
    "        \n",
    "        # Encode\n",
    "        all_embs, mean_emb = encoder(words_tokens.view(1, len(s)),\n",
    "                                     pos_tokens.view(1, len(s)), \n",
    "                                     len_s,\n",
    "                                     mask)\n",
    "        del(mask)\n",
    "        del(len_s)\n",
    "        del(pos_tokens)\n",
    "        del(words_tokens)\n",
    "\n",
    "        # Decode\n",
    "        start_token = torch.LongTensor([target_processor.w2i[START]])\n",
    "        predicted_words = []\n",
    "        \n",
    "        prediction = start_token.view(1,1)\n",
    "        hidden_state_batch = mean_emb\n",
    "\n",
    "        hidden_state_batch = hidden_state_batch.unsqueeze(0)\n",
    "        for w_idx in range(target_processor.max_sentence_length):# loop until EOS is produced or a max is reached (max_sentence_length)\n",
    "            prediction, hidden_state_batch,_ = decoder(prediction, # the previous prediction\n",
    "                                                       hidden_state_batch,\n",
    "                                                       all_embs,\n",
    "                                                       run_gpu=run_gpu)\n",
    "\n",
    "            index_predicted_word = np.argmax(prediction.detach().numpy(), axis=2)[0][0]\n",
    "            predicted_word = target_processor.i2w[index_predicted_word]\n",
    "            predicted_words.append(predicted_word)\n",
    "\n",
    "            if predicted_word == END:\n",
    "                break\n",
    "            \n",
    "            prediction = torch.LongTensor([index_predicted_word]).view(1,1)\n",
    "        \n",
    "        predicted_sentences.append(predicted_words)\n",
    "\n",
    "        del(start_token)\n",
    "        del(mean_emb)\n",
    "        del(hidden_state_batch)\n",
    "        del(all_embs)\n",
    "        del(prediction)\n",
    "    \n",
    "    with open(prediction_file_name, 'w', encoding='utf-8') as f:\n",
    "        for p in predicted_sentences:\n",
    "            if p[-1] == END:\n",
    "                p = p[:-1]\n",
    "            f.write(' '.join(p) + '\\n')\n",
    "    \n",
    "    # execute a powershell script for removing tokens\n",
    "    prediction_restored = prediction_file_name[:-5] + '_restored.pred'\n",
    "    _ = %ps get-content {prediction_file_name} | %{{$$_ -replace \"(@@ )|(@@ ?$)\",\"\"}} | out-file {prediction_restored} -encoding Ascii\n",
    "    perl_script = subprocess.Popen([\"C:/Strawberry/perl/bin/perl.exe\", \n",
    "                                    \"./tools/mosesdecoder/scripts/generic/multi-bleu.perl\", \n",
    "                                    \"-lc\", \n",
    "                                    \"./data/val/val_tokenized_lowercased.en\",\n",
    "                                    \"<\",prediction_restored],\n",
    "                                   shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    bleu_out,bleu_err = perl_script.communicate()\n",
    "    run_gpu=True\n",
    "    return bleu_out.decode(\"utf-8\"), bleu_err.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4db1c7c6974e8883ff0411f3c3d0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c525683ad134bd2a49bb4ef95a3aee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "opt.zero_grad()\n",
    "\n",
    "#loop over number of epochs\n",
    "for it in tnrange(1):\n",
    "    batch_losses = []\n",
    "    #loop over all the training batches\n",
    "    for i_batch, batch in tqdm_notebook(enumerate(trainloader), total=len(trainloader),leave=False):\n",
    "        image, caption, caption_lengths = transform_batch(batch, processor)\n",
    "        image = image.to(device)\n",
    "        loss = caption_model(image, caption, caption_lengths)\n",
    "        loss.backward()\n",
    "        batch_losses.append(float(loss))\n",
    "        opt.step()\n",
    "    losses += batch_losses\n",
    "    #create validation result file\n",
    "    caption_model.train(False)\n",
    "    #perform validation\n",
    "    timestamp = datetime.now()\n",
    "    prediction_file_name = 'val_epoch_{}_baseline_t_{:%m_%d_%H_%M}.pred'.format(it, timestamp)\n",
    "    blue,_ = validation_predictions(encdec,prediction_file_name)\n",
    "    print(\"Pseudo-Epoch {}:\\t{}\".format(pseudo_epoch,blue.strip()))\n",
    "    # Dump trained models\n",
    "    torch.save(encdec.state_dict(), 'encmean_model_it_{}_t_{:%m_%d_%H_%M}.torchsave'.format(it, timestamp))\n",
    "    if run_gpu:\n",
    "        encdec = encdec.cuda()\n",
    "    encdec.train(True)\n",
    "    \n",
    "    ## perform the coco evaluation provided by COCO. \n",
    "    # create coco object and cocoRes object# creat \n",
    "    algName = 'baseline_epoch%d'%it\n",
    "    [resFile, evalImgsFile, evalFile] = ['./results/captions_%s_%s_%s.json'%(dataType,algName,subtype) for subtype in subtypes]\n",
    "    coco = COCO(annFile)\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dump trained models\n",
    "# timestamp = datetime.now()\n",
    "# last_model_file_name = 'encmean_model_last-it_{}_t_{:%m_%d_%H_%M}.torchsave'.format(it, timestamp)\n",
    "# torch.save(encdec.state_dict(), last_model_file_name)\n",
    "\n",
    "# print('Model saved in file: {}'.format(last_model_file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
